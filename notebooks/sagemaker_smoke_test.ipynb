{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SageMaker Smoke Test\n",
        "\n",
        "This notebook validates that your SageMaker environment is properly configured for running Contemplative Constitutional AI experiments.\n",
        "\n",
        "It performs comprehensive tests including:\n",
        "- System requirements\n",
        "- PyTorch and CUDA setup\n",
        "- Model loading capabilities\n",
        "- S3 connectivity\n",
        "- Generation testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import psutil\n",
        "import yaml\n",
        "\n",
        "# Navigate to repo root\n",
        "os.chdir('..')\n",
        "sys.path.insert(0, str(Path.cwd() / 'src'))\n",
        "\n",
        "print(f\"Working directory: {os.getcwd()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. System Requirements Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== System Requirements Check ===\")\n",
        "print(f\"Python version: {sys.version}\")\n",
        "\n",
        "# Available memory\n",
        "memory = psutil.virtual_memory()\n",
        "print(f\"Total memory: {memory.total / (1024**3):.1f} GB\")\n",
        "print(f\"Available memory: {memory.available / (1024**3):.1f} GB\")\n",
        "print(f\"Memory usage: {memory.percent}%\")\n",
        "\n",
        "# Disk space\n",
        "disk = psutil.disk_usage('/')\n",
        "print(f\"Free disk space: {disk.free / (1024**3):.1f} GB\")\n",
        "\n",
        "# Check if we have enough memory\n",
        "if memory.available < 4 * (1024**3):  # 4GB minimum\n",
        "    print(\"⚠️ Warning: Less than 4GB available memory.\")\n",
        "else:\n",
        "    print(\"✅ Sufficient memory available\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. PyTorch and CUDA Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== PyTorch Installation Check ===\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# Check CUDA availability\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✅ CUDA is available with {torch.cuda.device_count()} GPU(s)\")\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"   GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "        props = torch.cuda.get_device_properties(i)\n",
        "        print(f\"   Memory: {props.total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"❌ CUDA is not available\")\n",
        "\n",
        "# Test tensor operations\n",
        "try:\n",
        "    x = torch.randn(3, 3)\n",
        "    print(f\"✅ CPU tensor creation successful: {x.shape}\")\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        x_cuda = x.to('cuda')\n",
        "        y_cuda = torch.randn(3, 3, device='cuda')\n",
        "        z_cuda = x_cuda + y_cuda\n",
        "        print(f\"✅ CUDA tensor operations successful: {z_cuda.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error in tensor operations: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Loader Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Model Loader Test ===\")\n",
        "\n",
        "from models.model_loader import ModelLoader\n",
        "\n",
        "try:\n",
        "    loader = ModelLoader()\n",
        "    print(\"✅ ModelLoader initialization successful\")\n",
        "    \n",
        "    # Test device detection\n",
        "    device = loader.detect_device()\n",
        "    print(f\"✅ Device detection successful: {device}\")\n",
        "    \n",
        "    # Test model info retrieval\n",
        "    model_info = loader.get_model_info('qwen2_0_5b')\n",
        "    print(f\"✅ Model info retrieval successful\")\n",
        "    print(f\"   Model: {model_info['model_name']}\")\n",
        "    print(f\"   Size: {model_info['model_size']}\")\n",
        "    print(f\"   Estimated memory: {model_info['estimated_memory_gb']}GB\")\n",
        "    \n",
        "    # Test loading config\n",
        "    loading_config = loader.get_loading_config(device)\n",
        "    print(f\"✅ Loading config retrieved for {device}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error in ModelLoader test: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Simple Model Loading Test\n",
        "\n",
        "This test loads a small, reliable model (GPT-2) to verify basic functionality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== Simple Model Loading Test ===\")\n",
        "\n",
        "try:\n",
        "    from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "    \n",
        "    print(\"Loading GPT-2 (small, reliable model)...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "    \n",
        "    print(\"✅ GPT-2 model loaded successfully\")\n",
        "    print(f\"   Model parameters: ~124M\")\n",
        "    print(f\"   Tokenizer vocab size: {len(tokenizer)}\")\n",
        "    \n",
        "    # Test generation\n",
        "    prompt = \"The meaning of life is\"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    \n",
        "    # Move to GPU if available\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = model.to(device)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=20,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    \n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"✅ Generation successful\")\n",
        "    print(f\"   Prompt: {prompt}\")\n",
        "    print(f\"   Response: {response}\")\n",
        "    \n",
        "    # Clean up\n",
        "    del model, tokenizer, outputs\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Simple model loading failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. S3 Connectivity Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== S3 Connectivity Test ===\")\n",
        "\n",
        "try:\n",
        "    from utils.sagemaker_utils import get_s3_client\n",
        "    import boto3\n",
        "    from botocore.exceptions import NoCredentialsError, ClientError\n",
        "    \n",
        "    # Load config\n",
        "    with open('configs/sagemaker_configs.yaml', 'r') as f:\n",
        "        sagemaker_config = yaml.safe_load(f)\n",
        "    \n",
        "    S3_BUCKET = sagemaker_config['s3']['bucket']\n",
        "    print(f\"Testing S3 bucket: {S3_BUCKET}\")\n",
        "    \n",
        "    s3_client = get_s3_client()\n",
        "    \n",
        "    # Test bucket access\n",
        "    s3_client.head_bucket(Bucket=S3_BUCKET)\n",
        "    print(f\"✅ Successfully accessed S3 bucket: {S3_BUCKET}\")\n",
        "    \n",
        "    # Test list operation\n",
        "    response = s3_client.list_objects_v2(Bucket=S3_BUCKET, MaxKeys=1)\n",
        "    print(\"✅ S3 list operation successful\")\n",
        "    \n",
        "except NoCredentialsError:\n",
        "    print(\"❌ AWS credentials not found\")\n",
        "except ClientError as e:\n",
        "    if e.response['Error']['Code'] == '404':\n",
        "        print(f\"❌ Bucket '{S3_BUCKET}' not found\")\n",
        "    else:\n",
        "        print(f\"❌ S3 error: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Unexpected error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. SageMaker Utilities Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=== SageMaker Utilities Test ===\")\n",
        "\n",
        "try:\n",
        "    from utils.sagemaker_utils import (\n",
        "        is_sagemaker_environment,\n",
        "        detect_sagemaker_device,\n",
        "        get_sagemaker_paths,\n",
        "        S3PathManager\n",
        "    )\n",
        "    \n",
        "    print(f\"Is SageMaker environment: {is_sagemaker_environment()}\")\n",
        "    print(f\"Detected device: {detect_sagemaker_device()}\")\n",
        "    \n",
        "    paths = get_sagemaker_paths()\n",
        "    print(f\"✅ SageMaker paths retrieved: {len(paths)} paths\")\n",
        "    \n",
        "    # Test S3PathManager\n",
        "    if S3_BUCKET != \"your-bucket-contemplative-ai\":\n",
        "        manager = S3PathManager(S3_BUCKET)\n",
        "        print(f\"✅ S3PathManager initialized\")\n",
        "        test_local = manager.get_local_path(\"test/file.txt\")\n",
        "        test_s3 = manager.get_s3_path(\"test/file.txt\")\n",
        "        print(f\"   Local path example: {test_local}\")\n",
        "        print(f\"   S3 path example: {test_s3}\")\n",
        "    \n",
        "    print(\"✅ All SageMaker utilities working\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error testing utilities: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Summary\n",
        "\n",
        "All tests complete! Check the results above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 50)\n",
        "print(\"Smoke Test Complete\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\\nIf all tests show ✅, your environment is fully configured!\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"  1. Run 00_quickstart.ipynb for end-to-end test\")\n",
        "print(\"  2. Use 01_data_generation.ipynb to create training data\")\n",
        "print(\"  3. Use 02_training.ipynb to train models\")\n",
        "print(\"  4. Use 03_evaluation.ipynb to evaluate results\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
